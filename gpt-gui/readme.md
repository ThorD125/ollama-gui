# this is a gui for ollama

## instructions

1. install ollama # see -> https://github.com/jmorganca/ollama
2. start ollama # ollama serve
3. run a model # ollama run llama2-uncensored
4. run the gui # python -m http.server

5. go to the gui
   http://127.0.0.1:8000
